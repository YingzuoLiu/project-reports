ğŸ¤– Chatbot + Recommendation System â€” Architecture, Training, and Enterprise Practices

A comprehensive study note summarizing key concepts in Chatbot Ã— Recsys Ã— RAG Ã— LLM Ã— Offline RL.
Author: Yingzuo Liu
Last Updated: 2025-11

ğŸ“˜ Overview

This repository summarizes my learning and understanding of Chatbot Ã— Recommendation System design, especially in e-commerce / customer service settings.

Covered topics include:

Retrieval-first Chatbot architecture (widely used in customer service)

Generation-first Chatbot architecture (LLM Agent style)

Multi-intent & multi-action scoring

RAG hallucination control

Dual-encoder retrieval, FAISS, MLP reranking

Multi-turn dialogue & intent drift

IPS / DR / SNIPS for unbiased offline evaluation

Latency optimization

Training methods: SFT / DPO / PPO / GRPO

Why dual-encoder instead of cross-encoder

How Chatbot integrates with Recsys design

This is a structured and interview-ready summary.

ğŸ› 1. Enterprise Chatbot Architectures

Two common enterprise designs:

A. Retrieval-first

B. Generation-first

Each serves a different purpose and is widely used in large-scale production systems.

ğŸ…° Retrieval-first Architectureï¼ˆæ£€ç´¢ä¼˜å…ˆï¼‰

ğŸ“Œ 90% of customer service systems use this.
Ideal for: FAQ, refund, logistics, policy QA, e-commerce customer support, hybrid recsys.

Core Idea

Retrieve first â†’ optional generation.
LLM is not the knowledge source. It only rewrites surfaces.

Pipeline
User Query
  â†“
NLU (intent classifier) + Encoder (text embedding)
  â†“
FAISS / Milvus vector retrieval (Top-K)
  â†“
Reranker (MLP or lightweight transformer)
  â†“
LLM converts retrieved knowledge to natural language (optional)

Advantages

High safety

Deterministic and controllable

No hallucination

Fast latency (ms-level)

Knowledge easily updated (update vector DB only)

Perfect fit for customer support & enterprise QA

Enterprise Characteristics

LLM is post-processing, not the decision-maker

MLP reranker is often enough (cheap & fast)

Strict filters for safety

Multi-intent & multi-action scoring for business logic

When to use

Refund/return policy

Logistics tracking

After-sale Q&A

Recsys recall + reranking

Corporate knowledge base search

Large-scale, low-latency production

ğŸ…± Generation-first Architectureï¼ˆç”Ÿæˆä¼˜å…ˆï¼‰

ğŸ“Œ Used for: LLM Agents, workflow automation, complex multi-turn reasoning.

Core Idea

LLM first plans â†’ then decides whether to retrieve, call tools, or act.

LLM acts as:

Planner

Reasoner

Tool orchestrator

State machine

Pipeline
User Query
  â†“
LLM performs planning + intent reasoning
  â†“
LLM decides:
    - retrieve or not?
    - call tools/APIs?
    - clarify missing info?
    - return final answer?
  â†“
Execute sub-steps (RAG / Tools)
  â†“
LLM integrates results and responds

Enterprise Characteristics

Designed for complex multi-turn interactions

Tool calling is core

Strong need for rule-engine + schema validation

Works well with automation workflows

More flexible, but less stable

When to use

Intelligent CS assistant (automated refund, auto-generate forms)

Multi-step tasks (address change + delivery change)

Complex business rules with conditional logic

Enterprise internal agent (Jira/SAP/Confluence integration)

ğŸ†š Retrieval-first vs Generation-first
Comparison	Retrieval-first	Generation-first
Philosophy	Select the correct answer	Plan and execute
Reliability	Very high	Medium (requires constraints)
Latency	Low	High
Hallucination	Near-zero	Possible
Multi-turn reasoning	Limited	Strong
Tool calling	Optional	Core
Recsys integration	Excellent	Decent
Knowledge updates	Fast	Requires retraining or prompts
Use Case	Customer support, FAQ, Recsys	Agent automation, multi-step tasks
Practical rule:

Need correctness â†’ Retrieval-first
Need reasoning/automation â†’ Generation-first

ğŸ§  2. Multi-intent & Multi-action Scoring

Customer queries often contain multiple intents:

â€œRefund + logistics checkâ€

â€œAddress change + compensationâ€

â€œReturn + stock availabilityâ€

Your approach:

Multi-label scoring
score(intent_i | query, history)
score(action_j | context, selected_intents)


Keep top intents & actions â†’ final decision uses business rules + confidence.

ğŸ” 3. Multi-turn Dialogue & Intent Drift

Intent changes over turns.

Inspired by DIN/DIEN idea:

Attention selects relevant past actions

Gating captures interest drift

Transformer > RNN for stability & long context

ğŸ” 4. Vector Retrieval (FAISS)
Why not cross-encoder?

Requires concatenation and transformer for each candidate â†’ slow

Cannot pre-build index

High latency

Why dual-encoder?

Build vector DB offline

Dot-product retrieval (1â€“3ms)

Scale to millions of items

Suitable for multilingual alignment

ğŸ¯ 5. Ranking (Why MLP is Enough?)

Embedding already encodes semantics.

MLP advantages:

microsecond inference

simple & robust

avoids heavy transformer reranker cost

common in YouTube, TikTok recommender stacks

ğŸ“š 6. RAG Hallucination Control

Three-layer safety mechanisms:

â‘  Semantic Confidence

Embedding margin too small â†’ uncertain.

â‘¡ Rule-based validation

Amount must be positive

Dates must be valid

Refund policy must match DB structure

â‘¢ Fallback to Human

Low score â†’ escalate to agent.

ğŸ§® 7. IPS / DR / SNIPSï¼ˆåå·®æ ¡æ­£ & Offline RLï¼‰

Interactions are biased: user only clicks what old system shows.

IPS

Importance sampling adjusts for old-policy bias.

DR

IPS + model estimation â†’ lower variance, more stable.

SNIPS

Normalized IPS to avoid huge weights.

Usage

Estimate new agent's reward safely

Adjust intent/action model learning

Reduce bias in recommendation-reranker training

âš™ï¸ 8. Latency Optimization

Practical tips you applied:

INT8/FP16 model quantization

FAISS IVF-PQ / HNSW

Redis warm vector cache

MLP reranker only

LLM called after retrieval

Context window control

ğŸ§ª 9. Training Strategies
SFT

Align tone, format, persona, safety.

DPO / ORPO

Align model to preference pairs (politeness, safety, correctness).

PPO / GRPO

Reinforcement learning:

reward = task completion + safety + customer satisfaction

GRPO minimizes KL drift + natural gradient updates

ğŸ§© 10. Combined System Architecture

Unified design combining both LLM and Recsys:

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        User Query            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        NLU + Encoder + Context Tracking
                          â†“
        Multi-intent & Multi-action Scoring
                          â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retrieval-first â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Vector DB â†’ FAISS â†’ MLP Rerank â†’ LLM    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generation-first â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ LLM Planning â†’ Tool Calling â†’ RAG       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                     Final Answer
                          â†“
                   Safety Validation
