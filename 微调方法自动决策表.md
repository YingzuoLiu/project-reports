# 微调方法自动决策表



## 一、快速决策流程（简洁版）

```
1. 任务类型：知识型（检索/FAQ） / 行为型（风格/安全） / 能力型（分类/生成/推理）？
2. 资源限制：显存（低/中/高），是否能做在线RL？
3. 数据情况：是否有偏好对？是否有高质量标注？是否只有少量指令样本？
4. 风险容忍：是否可接受模型改变内部分布（高风险）？是否必须保留原模型行为？

根据答案选择：
- 知识型 + 可检索：**RAG → 无需微调优先**
- 有偏好对（行为调整）：**DPO / ORPO（优先）**
- 少量域数据、显存低：**LoRA / Q-LoRA**
- 需要新能力或域分类：**SFT（小规模或混合）**
- 需要推理/agent能力：**RFT / RL（PPO）或DPO+RL混合**
```

---

## 二、决策表（按场景）

| 场景 | 推荐方法 | 优先级/说明 | 显存建议 |
|---|---:|---|---:|
| 企业知识问答（文档检索） | **RAG（检索-生成）** | 最低成本，降低幻觉 | 0–12GB |
| 输出需要特定风格/格式 | **Prompt + LoRA（小样本SFT）** | 先用 prompt，效果不够才 LoRA | 4–16GB |
| 行为对齐 / 安全（有人类偏好） | **DPO / ORPO** | 用偏好对直接优化，稳定且不做 RL | 8–48GB（LoRA 可降显存） |
| 分类 / 标签预测（标准监督） | **SFT（监督微调）** | 传统方法，简单高效 | 8–32GB |
| 代码生成 | **SFT（代码数据） + DPO** | 先学写，再用偏好提升质量 | 16–48GB |
| 推理强化（多步/数学） | **RFT（reasoning finetune）或RL** | 需要 chain-of-thought/特殊数据 | 24GB+ |
| 边缘/本地部署 | **蒸馏 / Q-LoRA / Distill + Quant** | 小模型或量化部署 | 4–12GB |

---

## 三、方法速览（优缺点与触发条件）

- **RAG**：优点—快速、低风险；缺点—需要检索基础设施。
  - 触发条件：任务以事实检索为主，信息量大且不断更新。

- **Prompt Engineering**：优点—零训练成本；缺点—不可持久化、对复杂任务有限。
  - 触发条件：格式化输出、少量规则化需求。

- **LoRA / Q-LoRA**：优点—显存友好、训练快；缺点—表达能力受限（取决 rank）。
  - 触发条件：显存受限、数据量中等、只需微调少量行为/风格。

- **SFT（Supervised Fine-Tuning）**：优点—直接、易解释；缺点—全量参数改动风险高。
  - 触发条件：需要模型学习新任务或显著改能力时。

- **DPO / ORPO（Preference Optimization）**：优点—不依赖 RL，直接用偏好对上优化；缺点—需要偏好数据。
  - 触发条件：有偏好对数据或能收集 A/B 选择数据时。

- **RL（PPO 等）**：优点—灵活（reward shaping）；缺点—实现复杂、不稳定，调参难。
  - 触发条件：工具调用、长期行为优化、需要复杂 reward 时。

- **Distillation**：优点—把大模型能力迁移给小模型；缺点—需额外训练时间、可能丢细节。
  - 触发条件：部署受限需要轻量模型。

---

## 四、常见配套建议（工程实践）

- **先在线下试 Prompt → RAG → LoRA → SFT → DPO**，按成本递增顺序尝试。
- **显存受限**：优先 LoRA 或 Q-LoRA（8-bit/4-bit），只更新少量参数。
- **偏好数据收集**：A/B 测试、人工标注对话对、线上点击/选择都可转为偏好对。
- **监控**：上线后持续监控幻觉率、违规率、用户满意度，并把线上反馈转偏好对用于后续 DPO。

---



